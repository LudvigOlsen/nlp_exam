{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for tf modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from scipy.stats import iqr\n",
    "import utipy as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_ASD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = \"\"\n",
    "data = pd.read_csv(dpath+\"All-Diagnoses-Adults-DK-Triangles.csv\")\n",
    "data[\"Observation ID\"] = list(range(len(data)))\n",
    "unique_ids = pd.read_csv(dpath+\"unique_IDs.csv\")\n",
    "unique_ids = unique_ids[[\"File\",\"Sub.File\",\"Study\",\"Diagnosis\",\"Subject\",\"unique_ID\"]]\n",
    "unique_ids.columns = [\"File\",\"Sub File\",\"Study\",\"Diagnosis\",\"Subject\",\"Unique ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INCLUDE_ASD:\n",
    "    data = data[data.Diagnosis != \"Asperger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique IDs to the data\n",
    "data = pd.merge(data,\n",
    "                unique_ids,\n",
    "                on=[\"File\",\"Sub File\",\"Study\",\"Diagnosis\",\"Subject\"], \n",
    "                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the columns relevant to the TensorFlow pipeline\n",
    "tfdata = data[data.Recovered == 0]\n",
    "tfdata = tfdata[[\"Unique ID\", \"Diagnosis\", \"Observation ID\", \"Transcript\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transcripts = tfdata[\"Transcript\"]\n",
    "# transcripts.to_csv(dpath + \"transcripts_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = tfdata[\"Transcript\"][2]\n",
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slash(s):\n",
    "    s = s.replace(' . / ',' . ')\n",
    "    s = s.replace(' / . ',' . ')\n",
    "    return s.replace(' / ',' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_slash(tfdata[\"Transcript\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphen(s):\n",
    "    s = s.replace(' - ',' ')\n",
    "    return s.replace('- ',' ') # Not within words\n",
    "remove_hyphen(tfdata[\"Transcript\"][103]+ \" computer-mekanisk-agtigt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_special_danish_chars(s):\n",
    "    s = s.replace('aa', 'å')\n",
    "    s = s.replace('Aa', 'Å')\n",
    "    s = s.replace('ae', 'æ')\n",
    "    s = s.replace('Ae', 'Æ')\n",
    "    s = s.replace('oe', 'ø')\n",
    "    s = s.replace('Oe', 'Ø')\n",
    "    return s\n",
    "enforce_special_danish_chars(\"Oeh, \"+tfdata[\"Transcript\"][201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(s): # This happens automatically in keep_allowed_characters()\n",
    "    s = s.replace('\\'', '')\n",
    "    s = s.replace('`', '')\n",
    "    s = s.replace('´', '')\n",
    "    s = s.replace('\\\"', '')\n",
    "    return s\n",
    "remove_apostrophe(\"han 'havde' ti ting med `sig´ da han \\\"fik\\\" nok \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_spaces(s):\n",
    "    return re.sub(r'\\s\\s+', ' ', s)\n",
    "decrease_spaces(\"  , han havde     men så fik hun han \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_punctuation(s):\n",
    "    s = s.replace(',', ' , ')\n",
    "    s = s.replace('.', ' . ')\n",
    "    s = s.replace('?', ' ? ')\n",
    "    s = s.replace('!', ' ! ')\n",
    "    s = decrease_spaces(s)\n",
    "    return s\n",
    "space_punctuation(tfdata[\"Transcript\"][199] + \"  ,men hvem? Ja, hvem kan! Nej det... må være nok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parantheses(s):\n",
    "    # Stuff in () was inserted by the transcriber\n",
    "    # so we wish to remove that\n",
    "    s = re.sub(r'\\(\\([^)]*\\)\\)', '', s)\n",
    "    s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "    s = s.replace(' )', ' ')\n",
    "    s = s.replace('( ', ' ')\n",
    "    return decrease_spaces(s)\n",
    "remove_parantheses(\"han (hende) var så() træls ((spiser)) men hun )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_allowed_characters(s):\n",
    "    return re.sub('[^A-Za-zæøåÆØÅ.,\\-!? ]', '', s)\n",
    "keep_allowed_characters(tfdata[\"Transcript\"][199] + \"  ,men 'hvem?'' `Ja´, hÄ´un-kønnet hvæm kan! Nej det... må være nok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata[\"Transcript\"][199] + \"  ,men 'hvem?'' `Ja´, hÄ´un-kønnet hvæm kan! Nej det... må være nok!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = replace_slash(s)\n",
    "    s = remove_hyphen(s)\n",
    "    s = remove_parantheses(s)\n",
    "    s = enforce_special_danish_chars(s)\n",
    "    s = keep_allowed_characters(s)\n",
    "    s = space_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(tfdata[\"Transcript\"][199] + \"  ,men 'hvem?'' `Ja´, hÄ´un-kønnet hvæm kan! Nej det... må være nok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_transcripts = [clean_text(str(s)) for s in tfdata[\"Transcript\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_transcripts[2000:2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata[\"Transcript\"] = clean_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata[\"Num Chars\"] = [\n",
    "    len(s) for s in tfdata[\"Transcript\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def describe_num_chars(nums):\n",
    "    return {\"median\":np.median(nums),\n",
    "            \"mean\":np.mean(nums),\n",
    "            \"std\":np.std(nums),\n",
    "            \"iqr\":iqr(nums),\n",
    "            \"min\":min(nums),\n",
    "            \"max\":max(nums)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_num_chars(tfdata[\"Num Chars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out those shorter than 40 chars\n",
    "tfdataFiltered = tfdata[tfdata['Num Chars'] > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_num_chars(tfdataFiltered[\"Num Chars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript(transcript, obs_id, size=210, stride = 40):\n",
    "\n",
    "    if len(transcript) < size:\n",
    "        return [[obs_id, transcript]]\n",
    "\n",
    "    # Convert to array of chars\n",
    "    def split_to_chars(sentence): \n",
    "        return [char for char in sentence]  \n",
    "    \n",
    "    transcript_chars = np.asarray(split_to_chars(transcript))\n",
    "    \n",
    "    naive_splits = ut.window(transcript_chars, size=size, gap=stride, rolling=True, discard_shorts = False)[0]\n",
    "    \n",
    "    def join_to_sentence(chars, is_first=False):\n",
    "        to_sent = \"\".join(list(chars))\n",
    "        if is_first:\n",
    "            start_at = 0\n",
    "            prefix = \"\"\n",
    "        else:\n",
    "            start_at = 1\n",
    "            prefix = \"... \"\n",
    "        \n",
    "        return prefix + \" \".join(to_sent.split(\" \")[start_at:-1])\n",
    "    \n",
    "    first_transcript = join_to_sentence(naive_splits[0], is_first=True)\n",
    "    rest = [join_to_sentence(s, is_first=False) for s in naive_splits[1:]]\n",
    "    transcript_splits = np.concatenate([[first_transcript], rest])\n",
    "    transcript_splits = [[obs_id, t] for t in transcript_splits]\n",
    "    \n",
    "    return transcript_splits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_string = \"fheh sdf h jehj fdsjh  sjhs aasdh  askjhd asdhj asd\"\n",
    "split_transcript(test_string, 3, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_row = tfdataFiltered[tfdataFiltered[\"Observation ID\"] == 56]\n",
    "problem_row\n",
    "split_transcript(problem_row[\"Transcript\"], problem_row[\"Observation ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_splits = [split_transcript(t, oi, size=210, stride=40) \\\n",
    "                     for oi, t in zip(tfdataFiltered[\"Observation ID\"], tfdataFiltered[\"Transcript\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript_splits = flatten(transcript_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transcript_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_splits[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_splits_df = pd.DataFrame.from_records(transcript_splits, columns=[\"Observation ID\", \"Transcript Split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_splits_df[\"Num Chars Split\"] = [\n",
    "    len(s) for s in transcript_splits_df[\"Transcript Split\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_splits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max transcript split length\n",
    "max(transcript_splits_df['Num Chars Split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out those shorter than 40 chars\n",
    "transcript_splits_df_filtered = transcript_splits_df[transcript_splits_df['Num Chars Split'] > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transcript_splits_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique IDs to the data\n",
    "tfdata_final = pd.merge(tfdataFiltered,\n",
    "                        transcript_splits_df_filtered,\n",
    "                        on=[\"Observation ID\"], \n",
    "                        how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata_final[\"Split ID\"] = list(range(len(tfdata_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata_final.to_csv(dpath + \"preprocessed_for_tf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
