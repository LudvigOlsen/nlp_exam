{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using NB, SVM, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utipy as ut\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_upper_limit = 3\n",
    "use_subwords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \n",
    "dpath = project_path + \"data/preprocessed/\"\n",
    "prefix = \"upsampled_\" # else \"\" \"upsampled_\" \"iscontrol_downsampled_\"\n",
    "data = pd.read_csv(dpath + prefix + \"grouped_for_tf.csv\")\n",
    "stopwords = list(pd.read_csv(project_path+\"stopwords_list_DK.txt\", header=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"isControl\"] = (data[\"Diagnosis\"] == \"Control\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_subwords(w, lower_limit, upper_limit):\n",
    "    w_len = len(w)\n",
    "    if (w_len < 1):\n",
    "        return []\n",
    "    upper_limit = min(upper_limit, w_len)\n",
    "    sizes = list(range(lower_limit, upper_limit+1))\n",
    "    if len(w) not in sizes:\n",
    "        sizes.append(w_len)\n",
    "    splits = flatten([ut.window(list(w), size=sz, discard_shorts=False)[0] \\\n",
    "            for sz in sizes])\n",
    "    add_hashtag = lambda t: \"##\" + t if t != w else t\n",
    "    tokens = [add_hashtag(\"\".join(t)) for t in splits]\n",
    "    return tokens\n",
    "\n",
    "split_to_subwords(\"monster\", 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def subword_tokenizer(x, lower_limit=2, upper_limit=3):\n",
    "    if not isinstance(x, list):\n",
    "        tokens = x.split(\" \")\n",
    "    else:\n",
    "        tokens = x\n",
    "    tokens = flatten([split_to_subwords(t, lower_limit, upper_limit) for t in tokens])\n",
    "    return tokens\n",
    "\n",
    "subword_tokenizer(\"Jeg er en dejlig kat !\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(x, stop_words=None, add_subwords=False, add_padding=False, pad_to=25):\n",
    "    x = x.lower()\n",
    "    tokens = x.split(\" \")\n",
    "    disallowed_tokens = [\".\",\"?\",\"!\",\",\",\"-\",\"...\"]\n",
    "    if stop_words is not None:\n",
    "        disallowed_tokens += stop_words\n",
    "    tokens = [t for t in tokens if t not in disallowed_tokens]\n",
    "    if add_subwords:\n",
    "        tokens = subword_tokenizer(tokens, 2,3)\n",
    "    if add_padding:\n",
    "        tokens += [\"PAD\"]*(pad_to-len(tokens))\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each sentence\n",
    "data[\"Transcript.Split\"] = [clean_sentence(trscpt, stop_words=stopwords, add_subwords=use_subwords) \\\n",
    "                            for trscpt in data[\"Transcript.Split\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing functions\n",
    "train_data = data[data[\"Fold\"] != 1]\n",
    "test_data = data[data[\"Fold\"] == 1]\n",
    "\n",
    "train_data[\"Transcript.Split\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(train_data[\"Transcript.Split\"])\n",
    "X_test = np.asarray(train_data[\"Transcript.Split\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=50000, ngram_range=(1, ngrams_upper_limit))\n",
    "X_train_bow = count_vec.fit_transform(X_train)\n",
    "X_test_bow = count_vec.transform(X_test)\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray([str(l) for l in train_data[\"Diagnosis\"]])\n",
    "y_test = np.asarray([str(l) for l in test_data[\"Diagnosis\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNBclf_bow = MultinomialNB()\n",
    "MNBclf_bow.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNBclf_bow = ComplementNB()\n",
    "CNBclf_bow.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_single(data, current_fold, fold_col=\"Fold\", classifiers={}):\n",
    "    \n",
    "    # Split data in train/test\n",
    "    train_data = data[data[fold_col] != current_fold]\n",
    "    test_data = data[data[fold_col] == current_fold]\n",
    "    \n",
    "    # Clean sentences - remove stopwords and some punctuation (done outside)\n",
    "    # X_train = np.asarray([clean_sentence(str(t), stop_words=stopword) \\\n",
    "    #                       for t in train_data[\"Transcript.Split\"]])\n",
    "    # X_test = np.asarray([clean_sentence(str(t), stop_words=stopwords) \\\n",
    "    #                      for t in test_data[\"Transcript.Split\"]])\n",
    "    X_train = np.asarray(train_data[\"Transcript.Split\"])\n",
    "    X_test = np.asarray(test_data[\"Transcript.Split\"])\n",
    "    \n",
    "    # Vectorize features (tokens)\n",
    "    count_vec = CountVectorizer(max_features=10000)\n",
    "    X_train_bow = count_vec.fit_transform(X_train)\n",
    "    X_test_bow = count_vec.transform(X_test)\n",
    "    \n",
    "    # Prepare labels for diagnosis classification\n",
    "    y_train_diagnosis = np.asarray([str(l) for l in train_data[\"Diagnosis\"]])\n",
    "    y_test_diagnosis = np.asarray([str(l) for l in test_data[\"Diagnosis\"]])\n",
    "    \n",
    "    # Prepare labels for isControl classification\n",
    "    y_train_iscontrol = np.asarray([l for l in train_data[\"isControl\"]])\n",
    "    y_test_iscontrol = np.asarray([l for l in test_data[\"isControl\"]])\n",
    "    \n",
    "    # Fit each classifier\n",
    "    fitted_models_diagnosis = {key:clf().fit(X_train_bow, y_train_diagnosis) for (key,clf) in classifiers.items()}\n",
    "    fitted_models_iscontrol = {key:clf().fit(X_train_bow, y_train_iscontrol) for (key,clf) in classifiers.items()}\n",
    "\n",
    "    # Predict test set with each classifier\n",
    "    predictions = pd.concat([pd.DataFrame({\n",
    "        \"Fold\":current_fold,\n",
    "        \"Classifier\":key, \n",
    "        \"DiagnosisPrediction\":clf.predict(X_test_bow),\n",
    "        \"IsControlPrediction\":fitted_models_iscontrol[key].predict(X_test_bow),\n",
    "        \"Target\":y_test_diagnosis,\n",
    "        \"isControl\":y_test_iscontrol,\n",
    "        \"Subwords\":int(use_subwords)}) \\\n",
    "                             for (key,clf) in fitted_models_diagnosis.items()])\n",
    "    \n",
    "    return predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_single(data, current_fold=1, fold_col=\"Fold\", classifiers={\"ComplementNB\": ComplementNB,\n",
    "                                                              \"MultinomialNB\": MultinomialNB,\n",
    "                                                              \"BernoulliNB\": BernoulliNB,\n",
    "                                                              \"RandomForestClassifier\": RandomForestClassifier,\n",
    "                                                              \"LinearSVC\":lambda : LinearSVC(dual=False, max_iter=3000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, fold_col=\"Fold\", classifiers={\"ComplementNB\": ComplementNB}):\n",
    "    return pd.concat([cv_single(data, current_fold=current_fold, \n",
    "                                fold_col=fold_col, classifiers=classifiers) \\\n",
    "                      for current_fold in np.unique(data[fold_col])]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(data, fold_col=\"Fold\", \n",
    "                            classifiers={\"ComplementNB\": ComplementNB,\n",
    "                                         \"MultinomialNB\": MultinomialNB,\n",
    "                                         \"BernoulliNB\": BernoulliNB,\n",
    "                                         \"RandomForestClassifier\": lambda : RandomForestClassifier(n_estimators=100),\n",
    "                                         \"LinearSVC\":lambda : LinearSVC(dual=False, max_iter=3000)\n",
    "                                         # \"MLPClassifier\": MLPClassifier # Too slow\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbw = \"_subwords_\" if use_subwords else \"_\"\n",
    "cv_results.to_csv(project_path+prefix+\"ngrams_\"+str(ngrams_upper_limit)+sbw+\"NB_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
